ğŸ§  Backpropagation

Backpropagation is how a neural network learns.
Simple idea:
Make a prediction
Compare with the true answer
Calculate how wrong it is (loss)
Move weights in the direction that makes the loss smaller
Repeat again and again
This is exactly how humans learn from mistakes.

â­ Step-by-Step Intuition 
1ï¸âƒ£ Network makes a prediction
Example:
Actual label = 1
Model predicts = 0.2
â†’ Bad prediction.

2ï¸âƒ£ Compute the loss
Example (Binary Cross Entropy):
Loss = high â†’ model is wrong.

3ï¸âƒ£ The model asks:
â€œWhich weight caused the mistake?â€
Different weights affect predictions differently.

4ï¸âƒ£ Compute gradients
A gradient tells:
direction to move
how much to move
Think of it like:
Hot â†’ go left
Cold â†’ go right
Gradients show the steepest direction to reduce loss.

5ï¸âƒ£ Update the weights
Using Gradient Descent:
new_weight = old_weight - learning_rate * gradient
Small learning rate â†’ slow learning
Huge learning rate â†’ unstable model

â­ Putting It All Together
Forward Pass:
input â†’ weight â†’ activation â†’ output
Backward Pass (Backprop):
output â†’ loss â†’ compute gradients â†’ update weights
Neural networks train by doing:
Forward â†’ Backward â†’ Forward â†’ Backward â†’ ... (many epochs)

â­ Example (Super Simple)
Consider 1 neuron:
output = w*x + b
Prediction vs actual â†’ find loss.
Then backprop adjusts:
weight (w)
bias (b)
So next time prediction becomes a little better.
After thousands of updates â†’ model becomes smart.